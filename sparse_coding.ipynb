{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the tutorial is to use Sparse Coding to understand the \"features\" which make-up handwritten digits. This sort of analysis is common when looking at natural signals (images) and also neuroscience data (LFP).\n",
    "\n",
    "Most of the code has already been written so you'll be running the code, looking at the plots, and manipulating some of the variables to understand how they affect the results. You should work in small groups (2-3) and go through the different sections of the notebook. Questions are highlighted in <font color='green'>green</font> throughout the notebook.\n",
    "\n",
    "We'll check-in occasionally to make sure everyone is making progress. Feel free to ask any of us questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IPython Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython notebooks are made up of a number of 'cells'. Each cell can either run some python code or contain text.\n",
    "\n",
    "To run a cell, click on it or move to it with up and down arrows and press 'Shift + Enter'. Alternatively, you can press the 'play' button in the toolbar. Any output from the cell (text or plots) will apprear below the cell after you run it. Try running the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can do much in Python, you'll probably need to import one or a few libraries. NumPy is a matrix library that is used in scientific python applications. The 'import X as Y' statement loads a library X under the alias Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython can tell you about the properties of functions and modules/libraries. One way to get this is to 'tab complete'. Given a library, 'np', you can query the function in the library by typing 'np.' and pressing 'Tab'. Give it a try! You need have imported numpy for this information to be available, so before trying this make sure you've executed the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about a function, you can type the function's name then a question mark and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.zeros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Only Sparse Coding on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements <font color='red'>**Positive Only**</font> <font color='purple'>**Sparse Coding**</font> on the <font color='blue'>**MNIST Dataset**</font>.\n",
    "\n",
    "Lets break that down! <font color='purple'>\n",
    "\n",
    "**Sparse Coding**</font> is an algorithm that seeks to disentangle the underlying generative factors of data. The result will be a set of \"dictionary elements\", akin to neuronal receptive fields, that represent these factors. \n",
    "<font color ='green'>**Can you guess what the factors would be for songs? for pictures? the emissions spectra of stars?**</font>\n",
    "<br><br>\n",
    "<font color='red'>**Positive-Only**</font> means that we don't allow our coefficients to be negative -- we prevent our neurons from firing \"negative spikes\" that correspond to the opposite of their receptive field. This makes the math a bit harder, but the results are easier to interpret.\n",
    "\n",
    "The dataset we are using is called <font color='blue'>**MNIST**</font>, and it's one of the most widely used datatsets in the field of machine learning. It's a collection of hand-written digits, drawn from American high school students and census takers.  <font color ='green'>**Can you think of any applications for an algorithm trained on this dataset?**</font>\n",
    "\n",
    "Intuitively speaking, the underlying generative factors of digits are pen strokes. That is, when these strokes are combined in the right way, we get digits. We will see that sparse coding discovers this underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Mathematical Interpretation\n",
    "In sparse coding, we define the following:\n",
    "\n",
    "$I$ is an image, $D$ is a dictionary of underlying generative factors (each with the same dimensionality as the input), and $A$ the sparse coefficients (one per dictionary element). \n",
    "\n",
    "In neuroscience terms, $I$ is our input from the retina, the dictionary $D$ is our set of receptive fields, and the coefficients $A$ are firing rates.\n",
    "\n",
    "We seek to write the *n*th image, $I_n$, as a weighted sum of the dictionary elements, $D_i$. \n",
    "\n",
    "Translating that into math gives us:\n",
    "\n",
    "$$I_n = D_1 * A_1(I_n) + D_2 * A_2(I_n) + \\ldots$$\n",
    "\n",
    "where the dictionary elements are the same for any image in the data set, and the coefficients, $A$, are different for each image. \n",
    "\n",
    "In order to learn the dictionary elements $D$, and the sparse coefficients, $A$, we minimize the following *error  function*, $E$:\n",
    "\n",
    "$$E = |I - \\sum_i A_i * D_i| ^ 2 + \\sum_i A_i $$\n",
    "\n",
    "where $$ |D_i|^2 = 1$$ and $$ A_i ≥ 0 $$\n",
    "\n",
    "Note that this is a linear algebraic equation. The matrix $D$ has the shape number of dictionary elements by the number of pixels, which are the lengths of the activation and image vectors, respectively.\n",
    "\n",
    "Let's look at it piece-by-piece.\n",
    "\n",
    "An *error function* is a mathematical construct that assigns an error value to any given state of our algorithm. The lower it is, the closer we are to achieving the goal of our algorithm -- that's why it's also called the *objective function*. It's a creature with many names$^†$!\n",
    "\n",
    "We have two competing goals, so we have two competing pieces in our error function.\n",
    "\n",
    "The first part, $|I - \\sum_i A_i * D_i| ^ 2$, takes the dictionary elements, multiplies them by the coefficient values, and then subtracts that away from the image. The result is thus the part of the image that we haven't yet explained, or the *representation error*, and it is also the *squared error*.$^*$\n",
    "\n",
    "The second part, $ \\sum_i A_i $, adds up all of our activations. The more active we are, the worse we do on our objective function. This is our *sparseness penalty*. \n",
    "\n",
    "The constraint on $|D_i|^2$ gets rid of a possible redundancy in our equation. Without it, a small dictionary element multiplied by a large sparse coefficient is the same as a large dictionary element multiplied by a small sparse coefficient. By restricting the magnitude of the dictionary elements, we make sure that all the information about intensity goes into the coefficients. The other constraint is our <font color='red'>*positive only* </font> constraint -- activations must be positive!\n",
    "\n",
    "To minimize this function, we do the following steps:\n",
    "\n",
    "0. Randomly initialize $D$.\n",
    "\n",
    "1. Choose a batch of images $I$.\n",
    "\n",
    "2. Minimize$^‡$ $E$ with respect to $A$, holding $D$ fixed.\n",
    "\n",
    "3. *Keeping that value of $A$*, change $D$ by a really small amount to reduce the value of $E$.\n",
    "\n",
    "4. Return to step 2.\n",
    "\n",
    "======  \n",
    "Footnotes:\n",
    "\n",
    "<font size ='1'> $^*$ Bonus question: <font color ='green'> why is the error squared? </font> (this is a question with more than one answer, and some of them are quite deep!)\n",
    "\n",
    "$^†$ There are many uses for this mathematical object, and each use has its own name. People who study decision-making call it a *cost function*. Economists and utilitarians flip it upside down and call it a *utility function* -- higher is better, so utility is mazimized. Physicists use them to describe physical systems, where they become an *energy function*, and energy is always minimized. There's a mathematical connection between all of these, but the math just expresses the way humans think -- magnets \"want\" to point north, while rational actors are \"compelled\" towards the optimal solution!\n",
    "<br> <br>\n",
    "$^‡$ You could do this with gradient descent, but we'll use a faster method called \"FISTA\" -- see the appendix to this document if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Interpretation\n",
    "In addition to being a useful tool for understanding data, sparse coding is also compelling because it provides a principle and a mechanism by which the sensory cortex might learn from stimuli.\n",
    "\n",
    "Step 3 is analagous to neurons integrating information $I$ and firing action potentials $A$. We call this the **Inference Step**. In the brain, this happens quickly in response to stimuli. Because the brain wishes to accurately represent the outside world, the neurons must seek to minimize the error of their representation, the first part of our equation. \n",
    "\n",
    "Because spiking is metabolically expensive (our big brains use up more than 25% of our calories!), we'd like to achieve a good representation with as few spikes as possible. If we do really well (fewer active neurons than pixels in the input), then we've also achieved compression, since our internal representation is smaller than the input. \n",
    "\n",
    "If you know a thing or two about information theory, then you might be astounded to discover that such a simple algorithm can discover compression schemes, which usually take very many, very clever computer scientists quite some time. You'll be even more astounded if you compare the results of sparse coding applied to natural images with the .PNG file format for storing images (look at the wavelets. They're incredibly similar!\n",
    "\n",
    "Step 4 is, then, analogous to synaptic plasticity. We call this the **Learning Step**. It causes changes in the neuronal receptive fields $D$ that allow those neurons to perform better at our stated objective -- the more images, the better. Another bonus question: <font color ='green'> what would happen if we just showed the same image over and over again? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Let's Get Started!\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "We'll need numpy (matrix library) and matplotlib (plotting library) to do most of the heavy lifting. Additionally, we'll use a few tools from scipy (scientific computing) and ones that we've written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # NUMerical PYthon library, matrix library\n",
    "import matplotlib # Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "from scipy.io import loadmat # load .mat files\n",
    "from scipy.stats import probplot, expon, norm, halfnorm # Stats functionality\n",
    "\n",
    "from rf_plot import show_fields # code for plotting\n",
    "\n",
    "from network import Network # code for sparse-coding network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load Data\n",
    "First, we'll load the data and do a little preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = \"mnist.mat\"\n",
    "\n",
    "data = loadmat(data_file)\n",
    "IMAGES = data['IMAGES']\n",
    "LABELS = data['LABELS']\n",
    "\n",
    "# Set basic parameters\n",
    "(K, L_img, L_img) = IMAGES.shape\n",
    "print('Number of images: '+str(K))\n",
    "K # Number of base images\n",
    "L_img # Linear size of images from the data\n",
    "N_pix_img = L_img ** 2\n",
    "\n",
    "# Scale the images to have a constant standard deviation\n",
    "IMAGES = IMAGES / np.std(IMAGES.astype(float), axis = (1, 2), keepdims = True)\n",
    "data = IMAGES.reshape(-1, N_pix_img)\n",
    "order = np.random.permutation(data.shape[0]) # Permute the data since they are sorted by digit\n",
    "data = data[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a few of the images from the dataset. Each images is a 14 by 14 image of a handwritten digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "show_fields(data[:81], cmap='gray_r', pos_only=True)\n",
    "plt.title('Images')\n",
    "p = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size = '3'> 1) Intuitively, what are handwritten digits 'made' out of?\n",
    "<br>\n",
    "2) Why is it okay to combine a collection of positive-only things to make handwritten digits?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Create Sparse Coding Network\n",
    "Set some parameters for training the network and create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_only = True # Positive Only Sparse coding if True\n",
    "N_sp = 81 # Number of sparse dictionary elements\n",
    "lamb = 0.5 # Sparsity parameter (0.5 is a good value, too large and you'll get NaNs)\n",
    "eta = 0.05 # Dictionary Learning Step Size\n",
    "\n",
    "net = Network(N_sp, N_pix_img, lamb, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the untrained dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "show_fields(net.D, cmap='gray_r',pos_only = True)\n",
    "plt.title('Dictionary')\n",
    "p = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A, errors = net.infer_A(data[0:100], n_g_steps=150, track_cost=True)\n",
    "plt.plot(np.log(errors))\n",
    "plt.title('Error During Inference')\n",
    "plt.xlabel('Iteration')\n",
    "p = plt.ylabel('Log-Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size='3'>1) How was the dictionary initialized? What does the dictionary mean? What is the biological analogue for vision?\n",
    "<br>\n",
    "2) What is the inference step supposed to do? Is it working?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network and visualize the dictionary elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[MSE_hist, sparsity_hist, SNR_hist, error_hist] = net.train(data, reset=False, batch_size=200, n_batches=50, eta = eta)\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(MSE_hist)\n",
    "plt.title('Squared error during training')\n",
    "plt.xlabel('batch #')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(sparsity_hist)\n",
    "plt.title('Sparsity penalty during training')\n",
    "plt.xlabel('batch #')\n",
    "plt.ylabel('sparsity')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(cost_hist)\n",
    "plt.title('Total Error during training')\n",
    "plt.xlabel('batch #')\n",
    "p = plt.ylabel('error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the dictionary elements. You can go back and run the training again (multiple times) and see how the plots change over time. Once all three plots above have become noisy horizontal lines, learning is complete. Since the right-most plot shows our total error, it is the most important one, and takes the longest to reach completion. You might need to run the training cell many times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "show_fields(net.D, cmap='gray_r', pos_only=True)\n",
    "plt.title('Dictionary')\n",
    "p = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size='3'>1) What happens to the squared error, sparsity penalty, and the error during training?\n",
    "<br>\n",
    "2) What do the dictionary elements look like? What happens if you run the training cell multiple times? What happens to the dictionary as you train it more?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a sparse-coding network tried to reconstruct its input, we should be able to visualize the reconstructions in more detail for a particular image.\n",
    "\n",
    "You can run the following cell multiple times and it will plot a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "q = np.random.randint(net.A.shape[0])\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Reconstruction')\n",
    "plt.imshow(net.reconstruct(net.X, net.A)[q].reshape(L_img, L_img),\n",
    "           interpolation = 'nearest',\n",
    "           cmap = 'gray_r', vmin = 0, vmax = net.X[q].reshape(L_img, L_img).max())\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Orignal Image')\n",
    "plt.imshow(net.X[q].reshape(L_img, L_img),\n",
    "           interpolation = 'nearest',\n",
    "           cmap = 'gray_r')\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.hist(net.A[q])\n",
    "\n",
    "sort_idx = np.argsort(net.A[q])[::-1]\n",
    "N_active = np.sum(net.A[q] > 0.0)\n",
    "active_idx = sort_idx[0:N_active]\n",
    "\n",
    "plt.title('Histogram of Sparse Coefficients \\n Number of active: %d' % N_active)\n",
    "plt.xlabel('Coefficient Value')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "show_fields(net.D[active_idx] * \n",
    "            net.A[q][active_idx][:, np.newaxis], \n",
    "            cmap = 'gray_r', pos_only = True)\n",
    "plt.title('Active Dictionary Elements \\n Scaled by their activations')\n",
    "p = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green', size='3'>1 ) How well is the network reconstructing the image?\n",
    "<br>\n",
    "2) How are the coefficient values distributed? Why are they distributed this way?\n",
    "<br>\n",
    "3) How is the bottom-right plot related to the reconstruction?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the joint statistics between different sparse coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_digits = 10000\n",
    "A = net.infer_A(data[np.random.randint(0, data.shape[0], n_digits), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n1, n2 = np.random.permutation(net.A.shape[1])[:2]\n",
    "_ = plt.hist2d(A[:,n1], A[:,n2], 20, norm=LogNorm())\n",
    "plt.colorbar()\n",
    "plt.xlabel('Neuron '+str(n2))\n",
    "p = plt.ylabel('Neuron '+str(n1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>1) How are the coefficients distributed? Everytime you run the plot, it will choose a different pair of neurons.\n",
    "<br>\n",
    "2) What would this plot look like if the coefficients were from a normal distribution?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the distribution of sparse coefficients (firing rates), we can compare them with different distribution. One tool for doing this is a probability plot. This plot compares the ordered value from the data to the ordered values the distribution. Closer to the red line is better (note the $R^2$ value at the bottom right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "As = net.A.ravel()\n",
    "p = probplot(As[As > 0], dist=expon, plot=plt)\n",
    "plt.xlabel('Ordered Values from Distribution')\n",
    "p = plt.ylabel('Ordered Values from Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>1) How well does the distribution line up with samples drawn from an exponential distribution?\n",
    "<br>\n",
    "2) How well does it match if you change \"expon\" to \"norm\" or \"halfnorm\"?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap<br>\n",
    "<font color='green' size ='3'> If you have time left go back and change the sparsity weight (lamb) to be larger or smaller. <br>\n",
    "1) What happens to the sparsity? What happens to the reconstruction error? <br>\n",
    "2) What do the dictionaries look like?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Appendix: Details on FISTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the objective function has an absolute value, typical gradient descent approaches converge slowly. Thus there are special purpose gradient descent methods that minimize functions that are in the form $$f(x) + g(x)$$ where $f(x)$ is a continuously differentiable, convex function and $g(x)$ is a convex, but not continuously differentiable function, such as $g(x) = \\alpha |x|$. One such method is called FISTA, or the Fast Iterative Shrinkage-Threshold Algorithm. \n",
    "\n",
    "The core kernel of the FISTA algorithm is the ISTA step:\n",
    "\n",
    "Define\n",
    "$$p_L(y) = \\text{argmin}_x \\, g(x) + L/2 * ||x- g(y)||^2$$ where $$g(y) = y - \\frac{1}{L} \\nabla f(y)$$\n",
    "\n",
    "and where $L$ is the constant such that $$||\\nabla f(x) - \\nabla f(y)|| \\le L ||x - y||$$\n",
    "\n",
    "When $g(x) = \\alpha|x|_1$, then $$p_L(y) = h_\\theta(g(y))\\qquad h_\\theta(y) = \\text{sign}(y)(|y|-\\theta)\\qquad \\theta = \\frac{\\alpha}{L}$$\n",
    "$h$ is applied pointwise its input and is called the shrinkage function. Simplying calculating $x_{t+1} = p_L(x_t)$ is the ISTA algorithm. If we more intelligently choose our new value to probe our function, then we get faster convergence. The FISTA algorithm is as follows:\n",
    "\n",
    "1. Initialize $y_0 = x_0 = X0$, $t_0=1$. \n",
    "\n",
    "2. For $k \\ge 0$, iterate the following:\n",
    "\n",
    "$$x_{k+1} = p_L(y_k)\\qquad t_{k+1} = 0.5 * (1 + \\sqrt{1 + 4 * t_k ^2})\\qquad y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}} * (x_{k+1} - x_k)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
