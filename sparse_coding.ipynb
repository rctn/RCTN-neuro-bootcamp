{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the tutorial is to use Sparse Coding to understand the \"features\" which make-up handwritten digits. This sort of analysis is common when looking at natural signals (images) and also neuroscience data (LFP).\n",
    "\n",
    "Most of the code has been written so you'll be running and the code, looking and the plots, and manipulating some of the variables to understand how they affect the results. You should work in small groups (2-3) and go through the different sections of the notebook. Questions are highlighted in <font color='green'>green</font> throughout the notebook.\n",
    "\n",
    "We'll check-in halfway through to make sure everyone is making progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IPython Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython notebooks are made up of a number of 'cells'. Each cell can either run some python code or contain text.\n",
    "\n",
    "To run a cell, click on it or move to it with up and down arrows and press 'Shift + Enter'. Alternatively, you can press the 'play' button in the toolbar. Any output from the cell (text or plots) will apprear below the cell after you run it Try running the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can do much in Python, you'll probably need to import one or a few libraries. NumPy is a matrix library that is used in scientific python applications. The 'import X as Y' statement loads a library X under the alias Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython can tell you about the properties of functions and modules/libraries. One way to get this is to 'tab complete'. Given a library, 'np', you can query the function in the library by typing 'np.' and pressing 'Tab'. Give it a try! You need have imported numpy for this information to be available, so before trying this make sure you've executed the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about a function, you can type the functions name then a question mark and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.zeros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Only Sparse Coding on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements Positive Only Sparse Coding on MNIST.\n",
    "\n",
    "Lets break that down.\n",
    "\n",
    "Sparse Coding is an algorithm that seeks to disentangle the underlying generative factors of data. In this particular case, we are considering the dataset, MNIST, which is a collection of digits. Intuitively speaking, the underlying generative factors of digits are pen strokes. When these strokes are combined in the right way, we get digits. We will see that sparse coding discovers this underlying structure. \n",
    "\n",
    "In sparse coding, we define the following:\n",
    "$I$ is an image, $D$ is a dictionary of underlying generative factors, and $A$ the sparse coefficients. We seek to write our image as a weighted sum of the dictionary elements. In equation form, \n",
    "\n",
    "$$I(x) = d_1(x) * a_1 + d_2(x) * a_2 + \\ldots$$\n",
    "\n",
    "where the dictionary elements are the same for any image in the data set, and the coefficients, $a$, are different for each image. \n",
    "\n",
    "In order to learn the dictionary elements $D$, and the sparse coefficients, $A$, we minimize the following objective function:\n",
    "\n",
    "$$E = |I - A * D| ^ 2 + |A|_1 \\qquad \\sum_x d_i(x)^2 = 1$$\n",
    "\n",
    "(Note $D$ is number of dictionary elements by the number of pixels). To minimize this function, we do the following steps:\n",
    "\n",
    "0. Choose an initial value of $D$.\n",
    "\n",
    "1. Choose a batch of images $I$.\n",
    "\n",
    "2. Minimize $E$ with respect to $A$ - we use FISTA which is a slighly better version of gradient descent.\n",
    "\n",
    "3. Keeping that value of $A$, change $D$ one gradient step to reduce the value of $E$.\n",
    "\n",
    "4. Return to step 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "from scipy.io import savemat, loadmat\n",
    "from scipy.stats import probplot, expon, norm, halfnorm\n",
    "\n",
    "from utils.rf_plot import show_fields\n",
    "\n",
    "from network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "data_file = \"mnist.mat\"\n",
    "output_dir = 'output/'\n",
    "\n",
    "data = loadmat(data_dir + data_file)\n",
    "IMAGES = data['IMAGES']\n",
    "LABELS = data['LABELS']\n",
    "\n",
    "# Set basic parameters\n",
    "(K, L_img, L_img) = IMAGES.shape\n",
    "print('Number of images: '+str(K))\n",
    "K # Number of base images\n",
    "L_img # Linear size of images from the data\n",
    "N_pix_img = L_img ** 2\n",
    "\n",
    "# Scale the images to have a constant standard deviation\n",
    "IMAGES = IMAGES / np.std(IMAGES.astype(float), axis = (1, 2), keepdims = True)\n",
    "data = IMAGES.reshape(-1, N_pix_img)\n",
    "order = np.random.permutation(data.shape[0]) # Permute the data since they are sorted by digit\n",
    "data = data[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Create Sparse Coding Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_only = True # Positive Only Sparse coding if True\n",
    "N_sp = 81 # Number of sparse dictionary elements\n",
    "lamb = 0.5 # Sparsity parameter (0.5 is a good value, too large and you'll get NaNs)\n",
    "eta = 0.05 # Dictionary Learning Step Size\n",
    "\n",
    "net = Network(N_sp, N_pix_img, lamb, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the functions in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the untrained dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_fields(net.D, pos_only = True)\n",
    "plt.title('Dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A, costs = net.infer_A(data[0:100], n_g_steps=150, track_cost=True)\n",
    "plt.plot(np.log(costs))\n",
    "plt.title('Cost During Inference')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>1) How was the dictionary initialized? What does the dictionary mean? What is the biological analogue for vision?\n",
    "<br>\n",
    "2) What is the inference step supposed to do? Is it working?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network and visualize the dictionary elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[MSE_hist, sparsity_hist, SNR_hist, cost_hist] = net.train(data, reset=False, batch_size=100, n_batches=100, eta = eta)\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(MSE_hist)\n",
    "plt.title('Mean squared error during training')\n",
    "plt.xlabel('batch #')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(sparsity_hist)\n",
    "plt.title('Sparsity during training')\n",
    "plt.xlabel('batch #')\n",
    "plt.ylabel('sparsity')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(cost_hist)\n",
    "plt.title('Cost during training')\n",
    "plt.xlabel('batch #')\n",
    "plt.ylabel('cost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the dictionary elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_fields(net.D, cmap='gray', pos_only=True)\n",
    "plt.title('Dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>1) What happens to MSE, sparsity, and the Cost during training?\n",
    "<br>\n",
    "2) What do the dictionary elements look like? What happens if you run the training cell multiple times? What happens to the dictionary as you train it more?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the reconstructions in more detail for a particular image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "q = np.random.randint(net.A.shape[0])\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Reconstruction')\n",
    "plt.imshow(net.reconstruct(net.X, net.A)[q].reshape(L_img, L_img),\n",
    "           interpolation = 'nearest',\n",
    "           cmap = plt.cm.gray, vmin = 0, vmax = net.X[q].reshape(L_img, L_img).max())\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Orignal Image')\n",
    "plt.imshow(net.X[q].reshape(L_img, L_img),\n",
    "           interpolation = 'nearest',\n",
    "           cmap = plt.cm.gray)\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.hist(net.A[q])\n",
    "\n",
    "sort_idx = np.argsort(net.A[q])[::-1]\n",
    "N_active = np.sum(net.A[q] > 0.0)\n",
    "active_idx = sort_idx[0:N_active]\n",
    "\n",
    "plt.title('Histogram of sparse Coefficients: \\n Number of active coefficients %d' % N_active)\n",
    "plt.xlabel('Coefficient Activity')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "show_fields(net.D[active_idx] * \n",
    "            net.A[q][active_idx][:, np.newaxis], \n",
    "            cmap = plt.cm.gray, pos_only = True)\n",
    "plt.title('Active Dictionary Elements \\n Scaled by their activations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>1) How well is the network reconstructing the image?\n",
    "<br>\n",
    "2) How are the coefficient values distributed? Why are they distributed this way?\n",
    "<br>\n",
    "3) How is the bottom-right plot related to the reconstruction?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the joint statistics between different sparse coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_digits = 10000\n",
    "A = net.infer_A(data[:n_digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n1, n2 = np.random.permutation(net.A.shape[1])[:2]\n",
    "_ = plt.hist2d(A[:,n1], A[:,n2], 20, norm=LogNorm())\n",
    "plt.colorbar()\n",
    "plt.xlabel('Neuron 2')\n",
    "plt.ylabel('Neuron 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>1) How are the coefficients distributed? Everytime you run the plot, it will choose different pairs of neurons.\n",
    "<br>\n",
    "2) How would this plot look if the coefficients were from a normal distribution?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the distribution of non-zero coefficients is exponentially distributed using a probability plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "As = net.A.ravel()\n",
    "p = probplot(As[As > 0], dist = expon, plot = plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>1) How well does the distribution line up with samples drawn from an exponential distribution?\n",
    "<br>\n",
    "2) How well does it match if you change \"expon\" to \"norm\" or \"halfnorm\"?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details on FISTA (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the objective function has an absolute value, typical gradient descent approaches converge slowly. Thus there are special purpose gradient descent methods that minimize functions that are in the form $$f(x) + g(x)$$ where $f(x)$ is a continuously differentiable, convex function and $g(x)$ is a convex, but not continuously differentiable function, such as $g(x) = \\alpha |x|$. One such method is called FISTA, or the Fast Iterative Shrinkage-Threshold Algorithm. \n",
    "\n",
    "The core kernel of the FISTA algorithm is the ISTA step:\n",
    "\n",
    "Define\n",
    "$$p_L(y) = \\text{argmin}_x \\, g(x) + L/2 * ||x- g(y)||^2$$ where $$g(y) = y - \\frac{1}{L} \\nabla f(y)$$\n",
    "\n",
    "and where $L$ is the constant such that $$||\\nabla f(x) - \\nabla f(y)|| \\le L ||x - y||$$\n",
    "\n",
    "When $g(x) = \\alpha|x|_1$, then $$p_L(y) = h_\\theta(g(y))\\qquad h_\\theta(y) = \\text{sign}(y)(|y|-\\theta)\\qquad \\theta = \\frac{\\alpha}{L}$$\n",
    "$h$ is applied pointwise its input and is called the shrinkage function. Simplying calculating $x_{t+1} = p_L(x_t)$ is the ISTA algorithm. If we more intelligently choose our new value to probe our function, then we get faster convergence. The FISTA algorithm is as follows:\n",
    "\n",
    "1. Initialize $y_0 = x_0 = X0$, $t_0=1$. \n",
    "\n",
    "2. For $k \\ge 0$, iterate the following:\n",
    "\n",
    "$$x_{k+1} = p_L(y_k)\\qquad t_{k+1} = 0.5 * (1 + \\sqrt{1 + 4 * t_k ^2})\\qquad y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}} * (x_{k+1} - x_k)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
