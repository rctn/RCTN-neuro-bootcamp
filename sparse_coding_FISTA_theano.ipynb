{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IPython Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython notebooks are made up of a number of 'cells'. Each cell can either run some python code or contain text.\n",
    "\n",
    "To run a cell, click on it or move to it with up and down arrows and press 'Shift + Enter'. Alternatively, you can press the 'play' button in the toolbar. Any output from the cell (text or plots) will apprear below the cell after you run it Try running the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can do much in Python, you'll probably need to import one or a few libraries. NumPy is a matrix library that is used in scientific python applications. The 'import X as Y' statement loads a library X under the alias Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython can tell you about the properties of functions and modules/libraries. One way to get this is to 'tab complete'. Given a library, 'np', you can query the function in the library by typing 'np.' and pressing 'Tab'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about a function, you can type the functions name then a question mark and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.zeros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Only Sparse Coding on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements Positive Only Sparse Coding on MNIST.\n",
    "\n",
    "Lets break that down.\n",
    "\n",
    "Sparse Coding is an algorithm that seeks to disentangle the underlying generative factors of data. In this particular case, we are considering the dataset, MNIST, which is a collection of digits. Intuitively speaking, the underlying generative factors of digits are pen strokes. When these strokes are combined in the right way, we get digits. We will see that sparse coding discovers this underlying structure. \n",
    "\n",
    "In sparse coding, we define the following:\n",
    "$I$ is an image, $D$ is a dictionary of underlying generative factors, and $A$ the sparse coefficients. In long form, \n",
    "\n",
    "$$I(x) = d_1(x) * a_1 + d_2(x) * a_2 + \\ldots$$\n",
    "\n",
    "where the dictionary elements are the same for any image in the data set, and the coefficients, $a$, are different for each image. \n",
    "\n",
    "In order to learn the dictionary elements $D$, and the sparse coefficients, $A$, we minimize the following objective function:\n",
    "\n",
    "$$E = |I - A * D| ^ 2 + |A|_1 \\qquad DD^T \\sim I$$\n",
    "\n",
    "(Note $D$ is number of dictionary elements by the number of pixels). To minimize this function, we do the following steps:\n",
    "\n",
    "0. Choose an initial value of $D$.\n",
    "\n",
    "1. Choose a batch of images $I$.\n",
    "\n",
    "2. Minimize $E$ with respect to $A$ - we use FISTA which is a slighly better version of gradient descent.\n",
    "\n",
    "3. Keeping that value of $A$, reduce $E$ by one gradient step in the direction of $\\nabla_D E$. \n",
    "\n",
    "4. Return to step 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import savemat, loadmat\n",
    "from scipy.stats import probplot, expon\n",
    "\n",
    "from utils.rf_plot import show_fields\n",
    "from utils.fista import fista_updates\n",
    "\n",
    "from network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "data_fil = \"mnist.mat\"\n",
    "output_dir = 'output/'\n",
    "\n",
    "data = loadmat(data_dir + data_fil)\n",
    "IMAGES = data['IMAGES']\n",
    "LABELS = data['LABELS']\n",
    "\n",
    "# Set basic parameters\n",
    "(K, L_img, L_img) = IMAGES.shape\n",
    "print('Number of images: '+str(K))\n",
    "K # Number of base images\n",
    "L_img # Linear size of images from the data\n",
    "N_pix_img = L_img ** 2\n",
    "\n",
    "# Scale Images to have equal standard deviations and pixels between 0 and 1\n",
    "IMAGES = IMAGES / np.std(IMAGES.astype(float), axis = (1, 2), keepdims = True)\n",
    "IMAGES = IMAGES / np.max(IMAGES)\n",
    "\n",
    "pos_only = True # Positive Only Sparse coding if True\n",
    "N_sp = 81 # Number of sparse dictionary elements\n",
    "lamb = 0.01 # coefficient for the sparse coefficients\n",
    "eta = 0.01 # Dictionary Learning Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sparse Coding Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network(N_sp, N_pix_img, lamb, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the functions in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_only = True # Positive Only Sparse coding if True\n",
    "\n",
    "def threshold(t_X):\n",
    "    \"\"\"\n",
    "    Threshold function\n",
    "    \"\"\"\n",
    "    return T.switch(t_X > 0., t_X, -0.0 * t_X )\n",
    "\n",
    "t_Alpha = T.scalar('Alpha')\n",
    "t_L = T.scalar('L')\n",
    "t_Eta = T.scalar('eta')\n",
    "t_D_std = T.scalar('D_std')\n",
    "\n",
    "t_I = theano.shared(IMAGES.reshape(K, N_pix_img).astype('float32'), 'I')\n",
    "t_D = theano.shared(D, 'D')\n",
    "t_A = theano.shared(A, 'A')\n",
    "t_I_idx = T.ivector('I_idx') # Indicies into t_I to select a batch of images\n",
    "\n",
    "t_E_rec = T.sum((t_I[t_I_idx] - T.dot(t_A, t_D)) ** 2); t_E_rec.name = 'E_rec'\n",
    "t_E_sp = t_Alpha * T.sum(T.abs_(t_A)); t_E_sp.name = 'E_sp'\n",
    "t_E = t_E_rec + t_E_sp; t_E.name = 'E'\n",
    "\n",
    "t_Signal = T.sum(t_I[t_I_idx] ** 2)\n",
    "t_SNR = t_Signal / t_E_rec # FIXME: Not quite right\n",
    "\n",
    "t_gED = T.grad(t_E_rec, t_D)\n",
    "\n",
    "costs = theano.function(inputs = [t_I_idx, t_Alpha],\n",
    "                        outputs = [t_E, t_E_rec, t_E_sp, t_SNR])\n",
    "\n",
    "# FIXME: Make dictionary learning positive only\n",
    "dictionary_learning_step = theano.function(\n",
    "    inputs = [t_Alpha, t_Eta, t_D_std, t_I_idx],\n",
    "    outputs = [t_E, t_E_rec, t_E_sp],\n",
    "    updates = [(t_D, row_norm(threshold(t_D - t_Eta * row_norm(t_gED))))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fist_updates = fista_updates(t_A, t_E_rec, t_Alpha, t_L)\n",
    "_, t_fista_X, t_T = fist_updates.keys()\n",
    "\n",
    "fista_step = theano.function(inputs = [t_Alpha, t_L, t_I_idx],\n",
    "                             outputs = [t_E, t_E_rec, t_E_sp, t_SNR],\n",
    "                             updates = fist_updates)\n",
    "def calculate_fista_L():\n",
    "    \"\"\"\n",
    "    Calculates the 'L' constant for FISTA for the dictionary in t_D.get_value()\n",
    "    \"\"\"\n",
    "    D = t_D.get_value()\n",
    "    try:\n",
    "        L = 2 * eigh(np.dot(D, D.T), eigvals_only=True, eigvals=(N_sp-1,N_sp-1))[0]\n",
    "    except ValueError:\n",
    "        L = (2 * std ** 2 * N_sp).astype('float32') # Upper bound on largest eigenvalue\n",
    "    return L\n",
    "\n",
    "def reset_fista_variables():\n",
    "    \"\"\"\n",
    "    Resets fista variables\n",
    "    \"\"\"\n",
    "    A0 = np.zeros_like(t_A.get_value()).astype(theano.config.floatX)\n",
    "    t_A.set_value(A0)\n",
    "    t_fista_X.set_value(A0)\n",
    "    t_T.set_value(np.array([1.]).astype(theano.config.floatX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(N_itr, Alpha, Eta, cost_list, N_g_itr = 150, show_costs = True):\n",
    "    \"\"\"\n",
    "    Code to train a sparse coding dictionary\n",
    "    N_itr - Number of iterations, a new batch of images for each\n",
    "    Alpha - Sparsity cost parameter... E = E_rec + Alpha + |A|_1\n",
    "    cost_list - list to which the cost at each iteration will be appended\n",
    "    N_g_itr - number of gradient steps in FISTA\n",
    "    show_costs - If true, print out costs every N_itr/10 iterations\n",
    "    Returns I_idx - indices corresponding to the most recent image batch,\n",
    "        to be used later in visualizations\n",
    "    \"\"\"\n",
    "    if show_costs:\n",
    "        print 'Iteration, E, E_rec, E_sp, SNR'\n",
    "    for i in range(N_itr):\n",
    "        I_idx = np.random.randint(K, size = N_bat).astype('int32')\n",
    "        reset_fista_variables()\n",
    "        L = calculate_fista_L()\n",
    "        for _ in range(N_g_itr):\n",
    "            E, E_rec, E_sp, SNR = fista_step(Alpha, L, I_idx)\n",
    "        dictionary_learning_step(Alpha, Eta, D_std, I_idx)\n",
    "        # \n",
    "        E, E_rec, E_sp, SNR = costs(I_idx, Alpha)\n",
    "        cost_list.append(E)\n",
    "        if ((i + 1) % (1 + N_itr / 10) == 0) and show_costs:\n",
    "            print i, E, E_rec, E_sp, SNR\n",
    "    return I_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I_idx = train(100, Alpha, 0.2, cost_list, show_costs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I_idx = train(100, Alpha, 0.1, cost_list, show_costs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I_idx = train(100, Alpha, 0.03, cost_list, show_costs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_idx = train(1, Alpha, 0.01, cost_list, show_costs=False, N_g_itr = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.array(cost_list)))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log(cost)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "show_fields(t_D.get_value(), cmap = plt.cm.gray, pos_only = True)\n",
    "#plt.savefig(output_dir + 'mnist_basis.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "q = np.random.randint(N_bat)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Reconstruction')\n",
    "plt.imshow(np.dot(t_A.get_value(), t_D.get_value())[q].reshape(L_pat, L_pat),\n",
    "           interpolation = 'nearest',\n",
    "           cmap = plt.cm.gray, vmin = 0, vmax = t_I.get_value()[I_idx][q].reshape(L_pat, L_pat).max())\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Orignal Image')\n",
    "plt.imshow(t_I.get_value()[I_idx][q].reshape(L_pat, L_pat),\n",
    "           interpolation = 'nearest',\n",
    "           cmap = plt.cm.gray)\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.hist(t_A.get_value()[q])\n",
    "\n",
    "sort_idx = np.argsort(t_A.get_value()[q])[::-1]\n",
    "N_active = np.sum(t_A.get_value()[q] > 0.0)\n",
    "active_idx = sort_idx[0:N_active]\n",
    "\n",
    "plt.title('Histogram of sparse Coefficients: \\n Number of active coefficients %d' % N_active)\n",
    "plt.xlabel('Coefficient Activity')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "show_fields(t_D.get_value()[active_idx] * \n",
    "            t_A.get_value()[q][active_idx][:, np.newaxis], \n",
    "            cmap = plt.cm.gray, pos_only = True)\n",
    "plt.title('Active Dictionary Elements \\n Scaled by their activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "As = t_A.get_value().ravel()\n",
    "probplot(As[As > 0], dist = expon, plot = plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details on FISTA:\n",
    "\n",
    "Since the objective function has an absolute value, typical gradient descent approaches converge slowly. Thus there are special purpose gradient descent methods that minimize functions that are in the form $$f(x) + g(x)$$ where $f(x)$ is a continuously differentiable, convex function and $g(x)$ is a convex, but not continuously differentiable function, such as $g(x) = \\alpha |x|$. One such method is called FISTA, or the Fast Iterative Shrinkage-Threshold Algorithm. \n",
    "\n",
    "The core kernel of the FISTA algorithm is the ISTA step:\n",
    "\n",
    "Define\n",
    "$$p_L(y) = \\text{argmin}_x \\, g(x) + L/2 * ||x- g(y)||^2$$ where $$g(y) = y - \\frac{1}{L} \\nabla f(y)$$\n",
    "\n",
    "and where $L$ is the constant such that $$||\\nabla f(x) - \\nabla f(y)|| \\le L ||x - y||$$\n",
    "\n",
    "When $g(x) = \\alpha|x|_1$, then $$p_L(y) = h_\\theta(y)\\qquad h_\\theta(y) = \\text{sign}(y)(|y|-\\theta)\\qquad \\theta = \\frac{\\alpha}{L}$$\n",
    "$h$ is applied pointwise to $y$ and is the shrinkage function. Simplying calculating $x_{t+1} = p_L(x_t)$ is the ISTA algorithm. If we more intelligently choose our new value to probe our function, then we get faster convergence. The FISTA algorithm is as follows:\n",
    "\n",
    "1. Initialize $y_0 = x_0 = X0$, $t_0=1$. \n",
    "\n",
    "2. For $k \\ge 0$, iterate the following:\n",
    "\n",
    "$$x_{k+1} = p_L(y_k)\\qquad t_{k+1} = 0.5 * (1 + \\sqrt{1 + 4 * t_k ^2})\\qquad y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}} * (x_{k+1} - x_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#savemat(output_dir + 'mnist_dictionary.mat', \n",
    "#        {'D': t_D.get_value(), 'Alpha': Alpha,\n",
    "#         'Algorithm': 'FISTA', 'Normalization': 'Equal Standard Deviation'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
